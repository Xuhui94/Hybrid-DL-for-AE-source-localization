{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "77e6232a-1c9a-4845-9eaa-4b72047b144a",
   "metadata": {},
   "source": [
    "# Step 1: Data Loading "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dbdd68a9-c4ac-462b-8f3e-eb9c779c9827",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_data(scaled_data, train_ratio=0.6):\n",
    "    # Extract features and labels for stratified splitting\n",
    "    features = scaled_data[:, :-2]\n",
    "    labels = df.iloc[:, -2:]  # assuming D and phi are the last two columns\n",
    "    \n",
    "    # Create a single label for stratified splitting\n",
    "    strat_labels = labels.astype(str).agg('-'.join, axis=1)\n",
    "    \n",
    "    # Split the data\n",
    "    train_indices, test_indices = train_test_split(np.arange(len(features)), test_size=0.4, stratify=strat_labels, random_state=42)\n",
    "    train = scaled_data[train_indices]\n",
    "    test = scaled_data[test_indices]\n",
    "    return train, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "930d72df-8e0b-4548-8ec3-fc6f3ed2df5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaled_data = preprocess_data2(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ed454395-c723-4c64-ba69-4cd9f2eeef81",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(699, 1313)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scaled_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "75d73f71-1cce-4231-9899-c0e18f9e819e",
   "metadata": {},
   "outputs": [
    {
     "ename": "InvalidIndexError",
     "evalue": "(slice(None, None, None), slice(None, -2, None))",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "File \u001b[0;32m~/miniconda3/envs/tensorflow/lib/python3.8/site-packages/pandas/core/indexes/base.py:3653\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3652\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 3653\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcasted_key\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3654\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "File \u001b[0;32m~/miniconda3/envs/tensorflow/lib/python3.8/site-packages/pandas/_libs/index.pyx:147\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m~/miniconda3/envs/tensorflow/lib/python3.8/site-packages/pandas/_libs/index.pyx:153\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: '(slice(None, None, None), slice(None, -2, None))' is an invalid key",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mInvalidIndexError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 14\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;66;03m#scaled_data = preprocess_data2(df)\u001b[39;00m\n\u001b[1;32m     13\u001b[0m scaled_data \u001b[38;5;241m=\u001b[39m df\n\u001b[0;32m---> 14\u001b[0m train, test \u001b[38;5;241m=\u001b[39m \u001b[43msplit_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mscaled_data\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     15\u001b[0m train_x, train_y, test_x, test_y \u001b[38;5;241m=\u001b[39m reshape_data(train, test)\n\u001b[1;32m     17\u001b[0m \u001b[38;5;66;03m# Define the mapping from distances and angles to labels\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[1], line 3\u001b[0m, in \u001b[0;36msplit_data\u001b[0;34m(scaled_data, train_ratio)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msplit_data\u001b[39m(scaled_data, train_ratio\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.6\u001b[39m):\n\u001b[1;32m      2\u001b[0m     \u001b[38;5;66;03m# Extract features and labels for stratified splitting\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m     features \u001b[38;5;241m=\u001b[39m \u001b[43mscaled_data\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[1;32m      4\u001b[0m     labels \u001b[38;5;241m=\u001b[39m df\u001b[38;5;241m.\u001b[39miloc[:, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m2\u001b[39m:]  \u001b[38;5;66;03m# assuming D and phi are the last two columns\u001b[39;00m\n\u001b[1;32m      6\u001b[0m     \u001b[38;5;66;03m# Create a single label for stratified splitting\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/tensorflow/lib/python3.8/site-packages/pandas/core/frame.py:3761\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mnlevels \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m   3760\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_getitem_multilevel(key)\n\u001b[0;32m-> 3761\u001b[0m indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3762\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_integer(indexer):\n\u001b[1;32m   3763\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m [indexer]\n",
      "File \u001b[0;32m~/miniconda3/envs/tensorflow/lib/python3.8/site-packages/pandas/core/indexes/base.py:3660\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3655\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merr\u001b[39;00m\n\u001b[1;32m   3656\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[1;32m   3657\u001b[0m     \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[1;32m   3658\u001b[0m     \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[1;32m   3659\u001b[0m     \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n\u001b[0;32m-> 3660\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_check_indexing_error\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3661\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/tensorflow/lib/python3.8/site-packages/pandas/core/indexes/base.py:5737\u001b[0m, in \u001b[0;36mIndex._check_indexing_error\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   5733\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_check_indexing_error\u001b[39m(\u001b[38;5;28mself\u001b[39m, key):\n\u001b[1;32m   5734\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_scalar(key):\n\u001b[1;32m   5735\u001b[0m         \u001b[38;5;66;03m# if key is not a scalar, directly raise an error (the code below\u001b[39;00m\n\u001b[1;32m   5736\u001b[0m         \u001b[38;5;66;03m# would convert to numpy arrays and raise later any way) - GH29926\u001b[39;00m\n\u001b[0;32m-> 5737\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m InvalidIndexError(key)\n",
      "\u001b[0;31mInvalidIndexError\u001b[0m: (slice(None, None, None), slice(None, -2, None))"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
    "from utils import load_data, preprocess_data, preprocess_data2, reshape_data\n",
    "\n",
    "fileloc = \"/Users/joshhuang/MATLAB/ONR/Localization/\"\n",
    "#filename = \"36_Locations_2.csv\"\n",
    "filename = \"35_Locations_1.csv\"\n",
    "\n",
    "# Load and preprocess the data\n",
    "df = load_data(fileloc, filename)\n",
    "scaled_data = preprocess_data2(df)\n",
    "train, test = split_data(scaled_data)\n",
    "train_x, train_y, test_x, test_y = reshape_data(train, test)\n",
    "\n",
    "# Define the mapping from distances and angles to labels\n",
    "points = np.array([\n",
    "    [-10, 0], [-8, 0], [-6, 0], [-4, 0], [-2, 0],\n",
    "    [-10, -2], [-8, -2], [-6, -2], [-4, -2], [-2, -2], [0, -2],\n",
    "    [-10, -4], [-8, -4], [-6, -4], [-4, -4], [-2, -4], [0, -4],\n",
    "    [-10, -6], [-8, -6], [-6, -6], [-4, -6], [-2, -6], [0, -6],\n",
    "    [-10, -8], [-8, -8], [-6, -8], [-4, -8], [-2, -8], [0, -8],\n",
    "    [-10, -10], [-8, -10], [-6, -10], [-4, -10], [-2, -10], [0, -10]\n",
    "])\n",
    "distances = np.sqrt(np.sum(points**2, axis=1))\n",
    "phis = np.arctan2(points[:, 1], points[:, 0])\n",
    "fixed_mapping = {tuple([d, phi]): i + 1 for i, (d, phi) in enumerate(zip(distances, phis))}\n",
    "\n",
    "# Convert train_y to labels using the mapping\n",
    "def find_label_with_tolerance(pair, mapping, tolerance=1e-2):\n",
    "    distance, phi = pair\n",
    "    for key in mapping.keys():\n",
    "        if abs(key[0] - distance) < tolerance and abs(key[1] - phi) < tolerance:\n",
    "            return mapping[key]\n",
    "    raise KeyError(f\"No match found for {pair}\")\n",
    "\n",
    "labels = np.array([find_label_with_tolerance(pair, fixed_mapping) for pair in train_y])\n",
    "\n",
    "# Flatten train_x and concatenate with labels\n",
    "train_x_flattened = train_x.reshape(train_x.shape[0], -1)  # Flatten train_x if it's not already in 2D\n",
    "data_to_save = np.hstack((labels.reshape(-1, 1), train_x_flattened))\n",
    "\n",
    "# Save the original data to a CSV file without augmentation\n",
    "original_data_filename = \"/Users/joshhuang/PythonFolder/ONR/Hybrid/TS-GAN/Training.csv\"\n",
    "pd.DataFrame(data_to_save).to_csv(original_data_filename, index=False, header=False)\n",
    "print(f\"Saved original data to {original_data_filename}\")\n",
    "\n",
    "# Define label ranges and target counts for augmentation\n",
    "label_ranges = [(1, 35)]\n",
    "target_count = 50  # Target number of records per label range\n",
    "\n",
    "def augment_data_with_noise(data, target_count, noise_level=0.01):\n",
    "    replicates = target_count // len(data)\n",
    "    remainder = target_count % len(data)\n",
    "    augmented_data = pd.concat([data] * replicates + [data.iloc[:remainder]])\n",
    "\n",
    "    # Identifying feature columns (assuming label is in the first column)\n",
    "    feature_columns = augmented_data.columns[40:]  # Adjust if label is not the first column\n",
    "\n",
    "    # Adding random noise only to feature columns\n",
    "    noise = np.random.normal(loc=0.0, scale=noise_level, size=(augmented_data.shape[0], len(feature_columns)))\n",
    "    augmented_data[feature_columns] += noise\n",
    "\n",
    "    return augmented_data\n",
    "\n",
    "\n",
    "# Function to replicate data to reach a target count\n",
    "def augment_data(data, target_count):\n",
    "    replicates = target_count // len(data)\n",
    "    remainder = target_count % len(data)\n",
    "    return pd.concat([data] * replicates + [data.iloc[:remainder]])\n",
    "\n",
    "\n",
    "def interpolate_samples(data, target_count):\n",
    "    additional_samples_needed = target_count - len(data)\n",
    "    new_samples = []\n",
    "    for _ in range(additional_samples_needed):\n",
    "        idx1, idx2 = np.random.choice(len(data), 2, replace=False)\n",
    "        interpolation = data.iloc[idx1] + (data.iloc[idx2] - data.iloc[idx1]) * np.random.random(size=data.shape[1])\n",
    "        new_samples.append(interpolation)\n",
    "    augmented_data = pd.concat([data] + new_samples, ignore_index=True)\n",
    "    return augmented_data\n",
    "\n",
    "\n",
    "# Process each label range\n",
    "for start, end in label_ranges:\n",
    "    # Find all unique labels within the current range\n",
    "    unique_labels_in_range = np.unique(data_to_save[:, 0][(data_to_save[:, 0] >= start) & (data_to_save[:, 0] <= end)])\n",
    "\n",
    "    # Initialize an empty DataFrame to hold all augmented data for this range\n",
    "    augmented_data_range = pd.DataFrame()\n",
    "\n",
    "    # Iterate through each unique label in the current range\n",
    "    for label in unique_labels_in_range:\n",
    "        # Filter data for the specific label\n",
    "        mask = (data_to_save[:, 0] == label)\n",
    "        filtered_data = data_to_save[mask]\n",
    "\n",
    "        # Augment data by replication if necessary\n",
    "        if len(filtered_data) < target_count:\n",
    "            filtered_data = augment_data_with_noise(pd.DataFrame(filtered_data), target_count)\n",
    "        else:\n",
    "            filtered_data = pd.DataFrame(filtered_data)\n",
    "\n",
    "        # Append the augmented data for this label to the range DataFrame\n",
    "        augmented_data_range = pd.concat([augmented_data_range, filtered_data], ignore_index=True)\n",
    "\n",
    "    # Save the aggregated augmented data for this label range to a CSV file\n",
    "    output_filename = \"/Users/joshhuang/PythonFolder/ONR/Hybrid/TS-GAN/Training1.csv\"\n",
    "    augmented_data_range.to_csv(output_filename, index=False, header=False)\n",
    "    print(f\"Saved augmented data for labels {start} to {end} to {output_filename}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec767f6d-03ab-4115-90d5-2eb431474aaa",
   "metadata": {},
   "source": [
    "## Back code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6a1f4ed6-061c-4581-8625-6856299e57a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved augmented data for labels 1 to 35 to Training_labels_1_to_35.csv\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from utils import load_data, preprocess_data, preprocess_data1, reshape_data\n",
    "\n",
    "fileloc = \"/Users/joshhuang/MATLAB/ONR/Localization/\"\n",
    "filename = \"36_Locations_1.csv\"\n",
    "\n",
    "# Load and preprocess the data\n",
    "df = load_data(fileloc, filename)\n",
    "scaled_data = preprocess_data1(df)\n",
    "train, test = split_data(scaled_data)\n",
    "train_x, train_y, test_x, test_y = reshape_data(train, test)\n",
    "\n",
    "# Define the mapping from distances and angles to labels\n",
    "points = np.array([\n",
    "    [-10, 0], [-8, 0], [-6, 0], [-4, 0], [-2, 0],\n",
    "    [-10, -2], [-8, -2], [-6, -2], [-4, -2], [-2, -2], [0, -2],\n",
    "    [-10, -4], [-8, -4], [-6, -4], [-4, -4], [-2, -4], [0, -4],\n",
    "    [-10, -6], [-8, -6], [-6, -6], [-4, -6], [-2, -6], [0, -6],\n",
    "    [-10, -8], [-8, -8], [-6, -8], [-4, -8], [-2, -8], [0, -8],\n",
    "    [-10, -10], [-8, -10], [-6, -10], [-4, -10], [-2, -10], [0, -10]\n",
    "])\n",
    "distances = np.sqrt(np.sum(points**2, axis=1))\n",
    "phis = np.arctan2(points[:, 1], points[:, 0])\n",
    "fixed_mapping = {tuple([d, phi]): i + 1 for i, (d, phi) in enumerate(zip(distances, phis))}\n",
    "\n",
    "# Convert train_y to labels using the mapping\n",
    "def find_label_with_tolerance(pair, mapping, tolerance=1e-2):\n",
    "    distance, phi = pair\n",
    "    for key in mapping.keys():\n",
    "        if abs(key[0] - distance) < tolerance and abs(key[1] - phi) < tolerance:\n",
    "            return mapping[key]\n",
    "    raise KeyError(f\"No match found for {pair}\")\n",
    "\n",
    "labels = np.array([find_label_with_tolerance(pair, fixed_mapping) for pair in train_y])\n",
    "\n",
    "# Flatten train_x and concatenate with labels\n",
    "train_x_flattened = train_x.reshape(train_x.shape[0], -1)  # Flatten train_x if it's not already in 2D\n",
    "data_to_save = np.hstack((labels.reshape(-1, 1), train_x_flattened))\n",
    "\n",
    "# Define label ranges and target counts for augmentation\n",
    "label_ranges = [(1, 35)]\n",
    "target_count = 100  # Target number of records per label range\n",
    "\n",
    "# Function to replicate data to reach a target count\n",
    "def augment_data(data, target_count):\n",
    "    replicates = target_count // len(data)\n",
    "    remainder = target_count % len(data)\n",
    "    return pd.concat([data] * replicates + [data.iloc[:remainder]])\n",
    "\n",
    "# Process each label range\n",
    "for start, end in label_ranges:\n",
    "    # Find all unique labels within the current range\n",
    "    unique_labels_in_range = np.unique(data_to_save[:, 0][(data_to_save[:, 0] >= start) & (data_to_save[:, 0] <= end)])\n",
    "\n",
    "    # Initialize an empty DataFrame to hold all augmented data for this range\n",
    "    augmented_data_range = pd.DataFrame()\n",
    "\n",
    "    # Iterate through each unique label in the current range\n",
    "    for label in unique_labels_in_range:\n",
    "        # Filter data for the specific label\n",
    "        mask = (data_to_save[:, 0] == label)\n",
    "        filtered_data = data_to_save[mask]\n",
    "\n",
    "        # Augment data by replication if necessary\n",
    "        if len(filtered_data) < target_count:\n",
    "            filtered_data = augment_data(pd.DataFrame(filtered_data), target_count)\n",
    "        else:\n",
    "            filtered_data = pd.DataFrame(filtered_data)\n",
    "\n",
    "        # Append the augmented data for this label to the range DataFrame\n",
    "        augmented_data_range = pd.concat([augmented_data_range, filtered_data], ignore_index=True)\n",
    "\n",
    "    # Save the aggregated augmented data for this label range to a CSV file\n",
    "    output_filename = f\"Training_labels_{start}_to_{end}.csv\"\n",
    "    augmented_data_range.to_csv(output_filename, index=False, header=False)\n",
    "    print(f\"Saved augmented data for labels {start} to {end} to {output_filename}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9e0a638-e0eb-4a1a-99e9-f4d37bdc1326",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
